# ==============================================================================
# Merged Lakeflow Source: ga4_reporting
# ==============================================================================
# This file is auto-generated by scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime, timedelta
from typing import (
    Any,
    Dict,
    Iterator,
    List,
)
import json
import os
import time

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None
        # Handle complex types
        if isinstance(field_type, StructType):
            # Validate input for StructType
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
            # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
            if value == {}:
                raise ValueError(
                    f"field in StructType cannot be an empty dict. Please assign None as the default value instead."
                )
            # For StructType, recursively parse fields into a Row
            field_dict = {}
            for field in field_type.fields:
                # When a field does not exist in the input:
                # 1. set it to None when schema marks it as nullable
                # 2. Otherwise, raise an error.
                if field.name in value:
                    field_dict[field.name] = parse_value(
                        value.get(field.name), field.dataType
                    )
                elif field.nullable:
                    field_dict[field.name] = None
                else:
                    raise ValueError(
                        f"Field {field.name} is not nullable but not found in the input"
                    )

            return Row(**field_dict)
        elif isinstance(field_type, ArrayType):
            # For ArrayType, parse each element in the array
            if not isinstance(value, list):
                # Handle edge case: single value that should be an array
                if field_type.containsNull:
                    # Try to convert to a single-element array if nulls are allowed
                    return [parse_value(value, field_type.elementType)]
                else:
                    raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
            return [parse_value(v, field_type.elementType) for v in value]
        elif isinstance(field_type, MapType):
            # Handle MapType - new support
            if not isinstance(value, dict):
                raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
            return {
                parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
                for k, v in value.items()
            }
        # Handle primitive types with more robust error handling and type conversion
        try:
            if isinstance(field_type, StringType):
                # Don't convert None to "None" string
                return str(value) if value is not None else None
            elif isinstance(field_type, (IntegerType, LongType)):
                # Convert numeric strings and floats to integers
                if isinstance(value, str) and value.strip():
                    # Handle numeric strings
                    if "." in value:
                        return int(float(value))
                    return int(value)
                elif isinstance(value, (int, float)):
                    return int(value)
                raise ValueError(f"Cannot convert {value} to integer")
            elif isinstance(field_type, FloatType) or isinstance(field_type, DoubleType):
                # New support for floating point types
                if isinstance(value, str) and value.strip():
                    return float(value)
                return float(value)
            elif isinstance(field_type, DecimalType):
                # New support for Decimal type
                from decimal import Decimal

                if isinstance(value, str) and value.strip():
                    return Decimal(value)
                return Decimal(str(value))
            elif isinstance(field_type, BooleanType):
                # Enhanced boolean conversion
                if isinstance(value, str):
                    lowered = value.lower()
                    if lowered in ("true", "t", "yes", "y", "1"):
                        return True
                    elif lowered in ("false", "f", "no", "n", "0"):
                        return False
                return bool(value)
            elif isinstance(field_type, DateType):
                # New support for DateType
                if isinstance(value, str):
                    # Try multiple date formats
                    for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                        try:
                            return datetime.strptime(value, fmt).date()
                        except ValueError:
                            continue
                    # ISO format as fallback
                    return datetime.fromisoformat(value).date()
                elif isinstance(value, datetime):
                    return value.date()
                raise ValueError(f"Cannot convert {value} to date")
            elif isinstance(field_type, TimestampType):
                # Enhanced timestamp handling
                if isinstance(value, str):
                    # Handle multiple timestamp formats including Z and timezone offsets
                    if value.endswith("Z"):
                        value = value.replace("Z", "+00:00")
                    try:
                        return datetime.fromisoformat(value)
                    except ValueError:
                        # Try additional formats if ISO format fails
                        for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                            try:
                                return datetime.strptime(value, fmt)
                            except ValueError:
                                continue
                elif isinstance(value, (int, float)):
                    # Handle Unix timestamps
                    return datetime.fromtimestamp(value)
                elif isinstance(value, datetime):
                    return value
                raise ValueError(f"Cannot convert {value} to timestamp")
            else:
                # Check for custom UDT handling
                if hasattr(field_type, "fromJson"):
                    # Support for User Defined Types that implement fromJson
                    return field_type.fromJson(value)
                raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            # Add context to the error
            raise ValueError(
                f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}"
            )


    ########################################################
    # sources/ga4_reporting/ga4_reporting.py
    ########################################################

    try:
        from google.oauth2 import service_account
        from google.auth.transport.requests import Request
        GOOGLE_AUTH_AVAILABLE = True
    except ImportError:
        GOOGLE_AUTH_AVAILABLE = False


    class LakeflowConnect:
        def __init__(self, options: Dict[str, str]) -> None:
            """
            Initialize the GA4 Reporting connector with API credentials.

            Args:
                options: Dictionary containing:
                    - service_account_json: Google Service Account credentials as:
                        * JSON string (escaped)
                        * JSON object (dict)
                        * File path to .json file (string ending with .json)
                    - property_id: GA4 property ID (e.g., "123456789")
                    - initial_date: Initial date for first-time ingestion (default: "2024-01-01")
                    - end_date: End date for reports (default: "yesterday")
                    - report_configs: Optional custom report configurations
            """
            self.property_id = options["property_id"]
            self.initial_date = options.get("initial_date", "2024-01-01")
            self.end_date = options.get("end_date", "yesterday")
            self.lookback_days = int(options.get("lookback_days", 3))  # Default 3-day lookback for data freshness
            self.report_configs = options.get("report_configs", self._build_default_report_configurations())

            # Parse service account credentials from various input formats
            self.service_account_info = self._parse_service_account_credentials(
                options.get("service_account_json", options)
            )

            # Get access token
            self.access_token = self._get_access_token()
            self.token_expiry = None

            # Base URL for GA4 Data API
            self.base_url = "https://analyticsdata.googleapis.com/v1beta"

            # Build report configurations
            self._report_configs = self._build_default_report_configurations()

        def _parse_service_account_credentials(self, credentials_input) -> Dict:
            """
            Parse service account credentials from various input formats.

            Args:
                credentials_input: Can be:
                    - A JSON string (escaped)
                    - A dictionary (already parsed JSON)
                    - A file path to a .json file

            Returns:
                Dictionary containing service account credentials

            Raises:
                ValueError: If credentials cannot be parsed
            """
            # Case 1: Already a dictionary with service account fields
            if isinstance(credentials_input, dict):
                # Check if it looks like service account credentials directly
                if "type" in credentials_input and credentials_input.get("type") == "service_account":
                    return credentials_input
                # Check if service_account_json is nested in the dict
                elif "service_account_json" in credentials_input:
                    return self._parse_service_account_credentials(
                        credentials_input["service_account_json"]
                    )
                else:
                    raise ValueError(
                        "Invalid credentials format: dictionary must contain 'type': 'service_account' "
                        "or 'service_account_json' key"
                    )

            # Case 2: String - could be JSON or file path
            elif isinstance(credentials_input, str):
                # Check if it's a file path
                if credentials_input.endswith(".json"):
                    try:
                        with open(credentials_input, "r") as f:
                            return json.load(f)
                    except FileNotFoundError:
                        raise ValueError(
                            f"Service account JSON file not found: {credentials_input}"
                        )
                    except json.JSONDecodeError as e:
                        raise ValueError(
                            f"Invalid JSON in service account file {credentials_input}: {str(e)}"
                        )

                # Try to parse as JSON string
                try:
                    parsed = json.loads(credentials_input)
                    if isinstance(parsed, dict) and parsed.get("type") == "service_account":
                        return parsed
                    else:
                        raise ValueError(
                            "Parsed JSON does not contain valid service account credentials"
                        )
                except json.JSONDecodeError as e:
                    raise ValueError(
                        f"service_account_json must be valid JSON string, file path, or dict. "
                        f"Parse error: {str(e)}"
                    )

            else:
                raise ValueError(
                    f"service_account_json must be a string or dict, got {type(credentials_input)}"
                )

        def build_report_configurations(self, report_configs: Dict) -> None:
            """
            Build and return GA4 report configurations.

            Args:
                report_configs: Dictionary containing report configurations
            """
            self._report_configs = report_configs

        def _build_default_report_configurations(self) -> Dict:
            """
            Build and return GA4 report configurations.

            Each report configuration defines:
            - dimensions: List of dimension objects to group by
            - metrics: List of metric objects to aggregate
            - primary_key: List of dimension names that form the composite key

            Returns:
                Dictionary mapping report names to their configurations
            """
            return {
                "basic_report": {
                    "dimensions": [
                        {"name": "date"},
                        {"name": "sessionSource"},
                        {"name": "sessionMedium"},
                    ],
                    "metrics": [
                        {"name": "activeUsers"},
                        {"name": "sessions"},
                        {"name": "screenPageViews"},
                        {"name": "bounceRate"},
                        {"name": "averageSessionDuration"},
                    ],
                    "primary_key": ["date", "sessionSource", "sessionMedium"],
                },
                "user_demographics": {
                    "dimensions": [
                        {"name": "date"},
                        {"name": "country"},
                        {"name": "city"},
                        {"name": "userGender"},
                        {"name": "userAgeBracket"},
                    ],
                    "metrics": [
                        {"name": "activeUsers"},
                        {"name": "newUsers"},
                        {"name": "sessions"},
                    ],
                    "primary_key": ["date", "country", "city", "userGender", "userAgeBracket"],
                },
                "traffic_sources": {
                    "dimensions": [
                        {"name": "date"},
                        {"name": "sessionSource"},
                        {"name": "sessionMedium"},
                        {"name": "sessionCampaignName"},
                    ],
                    "metrics": [
                        {"name": "activeUsers"},
                        {"name": "sessions"},
                        {"name": "conversions"},
                        {"name": "totalRevenue"},
                    ],
                    "primary_key": ["date", "sessionSource", "sessionMedium", "sessionCampaignName"],
                },
                "page_performance": {
                    "dimensions": [
                        {"name": "date"},
                        {"name": "pagePath"},
                        {"name": "pageTitle"},
                    ],
                    "metrics": [
                        {"name": "screenPageViews"},
                        {"name": "averageSessionDuration"},
                        {"name": "bounceRate"},
                    ],
                    "primary_key": ["date", "pagePath"],
                },
                "event_data": {
                    "dimensions": [
                        {"name": "date"},
                        {"name": "eventName"},
                        {"name": "sessionSource"},
                    ],
                    "metrics": [
                        {"name": "eventCount"},
                        {"name": "eventValue"},
                        {"name": "conversions"},
                    ],
                    "primary_key": ["date", "eventName", "sessionSource"],
                },
            }

        def _get_access_token(self) -> str:
            """
            Get access token using service account credentials.

            Returns:
                Access token string
            """
            if GOOGLE_AUTH_AVAILABLE:
                # Use google-auth library if available (preferred method)
                scopes = ["https://www.googleapis.com/auth/analytics.readonly"]
                credentials = service_account.Credentials.from_service_account_info(
                    self.service_account_info, scopes=scopes
                )
                credentials.refresh(Request())
                self.token_expiry = credentials.expiry
                return credentials.token
            else:
                # Fallback to manual JWT creation if google-auth not available
                return self._create_jwt_token()

        def _create_jwt_token(self) -> str:
            """
            Create JWT token manually for service account authentication.
            This is a fallback method when google-auth library is not available.

            Returns:
                Access token string
            """
            import hashlib
            import hmac

            # JWT Header
            header = {"alg": "RS256", "typ": "JWT"}
            header_encoded = base64.urlsafe_b64encode(
                json.dumps(header).encode()
            ).decode().rstrip("=")

            # JWT Claim Set
            now = int(time.time())
            claim = {
                "iss": self.service_account_info["client_email"],
                "scope": "https://www.googleapis.com/auth/analytics.readonly",
                "aud": "https://oauth2.googleapis.com/token",
                "exp": now + 3600,
                "iat": now,
            }
            claim_encoded = base64.urlsafe_b64encode(
                json.dumps(claim).encode()
            ).decode().rstrip("=")

            # Create signature
            message = f"{header_encoded}.{claim_encoded}"

            # Import cryptography for RSA signing
            try:
                from cryptography.hazmat.primitives import hashes, serialization
                from cryptography.hazmat.primitives.asymmetric import padding
                from cryptography.hazmat.backends import default_backend

                # Load private key
                private_key_str = self.service_account_info["private_key"]
                private_key = serialization.load_pem_private_key(
                    private_key_str.encode(), password=None, backend=default_backend()
                )

                # Sign the message
                signature = private_key.sign(
                    message.encode(),
                    padding.PKCS1v15(),
                    hashes.SHA256()
                )

                signature_encoded = base64.urlsafe_b64encode(signature).decode().rstrip("=")

                # Combine to create JWT
                jwt_token = f"{message}.{signature_encoded}"

                # Exchange JWT for access token
                token_url = "https://oauth2.googleapis.com/token"
                data = {
                    "grant_type": "urn:ietf:params:oauth:grant-type:jwt-bearer",
                    "assertion": jwt_token,
                }

                response = requests.post(token_url, data=data)

                if response.status_code != 200:
                    raise Exception(
                        f"Failed to get access token: {response.status_code} {response.text}"
                    )

                token_data = response.json()
                self.token_expiry = datetime.now() + timedelta(seconds=token_data.get("expires_in", 3600))
                return token_data["access_token"]

            except ImportError:
                raise Exception(
                    "Neither google-auth nor cryptography library is available. "
                    "Please install one of them: 'pip install google-auth' or 'pip install cryptography'"
                )

        def _refresh_token_if_needed(self):
            """
            Refresh the access token if it's expired or about to expire.
            """
            if self.token_expiry is None or datetime.now() >= self.token_expiry - timedelta(minutes=5):
                self.access_token = self._get_access_token()

        def list_tables(self) -> List[str]:
            """
            List available GA4 report types.

            Returns:
                List of report type names
            """
            return list(self._report_configs.keys())

        def get_table_schema(self, table_name: str) -> StructType:
            """
            Get the Spark schema for a GA4 report.

            Args:
                table_name: Name of the report type

            Returns:
                StructType representing the report schema
            """
            if table_name not in self._report_configs:
                raise ValueError(f"Unknown report type: {table_name}")

            config = self._report_configs[table_name]

            # Build schema fields
            fields = []

            # Add composite key field
            fields.append(StructField("_composite_key", StringType(), False))

            # Add dimension fields
            for dim in config["dimensions"]:
                fields.append(StructField(dim["name"], StringType(), True))

            # Add metric fields - we'll determine the type dynamically
            # For now, use DoubleType for all metrics as they can be decimal values
            for metric in config["metrics"]:
                # Some metrics are inherently long integers (counts, users)
                if metric["name"] in [
                    "activeUsers",
                    "sessions",
                    "screenPageViews",
                    "eventCount",
                    "conversions",
                    "newUsers",
                ]:
                    fields.append(StructField(metric["name"], LongType(), True))
                else:
                    # Rates, durations, revenue are doubles
                    fields.append(StructField(metric["name"], DoubleType(), True))

            return StructType(fields)

        def read_table_metadata(self, table_name: str) -> Dict:
            """
            Get metadata for a GA4 report.

            Args:
                table_name: Name of the report type

            Returns:
                Dictionary with primary_key, cursor_field, and ingestion_type
            """
            if table_name not in self._report_configs:
                raise ValueError(f"Unknown report type: {table_name}")

            # All GA4 reports use CDC ingestion to handle data freshness window
            # The lookback window ensures we re-ingest last N days to capture late-arriving events
            # CDC mode ensures only latest version of each row is kept (deduplication by primary_key)
            return {
                "primary_key": "_composite_key",
                "cursor_field": "date",
                "ingestion_type": "cdc"
            }

        def read_table(
            self, table_name: str, start_offset: Dict
        ) -> tuple[Iterator[Dict], Dict]:
            """
            Read data from a GA4 report incrementally with lookback window.

            The lookback window ensures we re-ingest recent data to capture:
            - Late-arriving events (GA4 24-48 hour processing window)
            - Data corrections and updates

            CDC ingestion mode ensures only the latest version of each row is kept.

            Args:
                table_name: Name of the report type
                start_offset: Dictionary containing the last ingested date
                    - For incremental: {"date": "YYYYMMDD"}
                    - For initial load: {} or None

            Returns:
                Tuple of (records iterator, new offset dict with max date)
            """
            if table_name not in self._report_configs:
                raise ValueError(f"Unknown report type: {table_name}")

            # Determine start date for this incremental read
            if start_offset and start_offset.get("date"):
                # Incremental with lookback: go back N days from last checkpoint
                # This ensures we re-ingest recent data to capture late-arriving events
                last_date = start_offset["date"]
                start_date = self._subtract_days(last_date, self.lookback_days)
            else:
                # Initial load: use configured initial_date
                start_date = self.initial_date

            # Fetch data from start_date to end_date
            records_iter, max_date = self._fetch_report_data_incremental(
                table_name, start_date
            )

            # Return new offset with max date seen
            # This will be the checkpoint for next run
            new_offset = {"date": max_date} if max_date else {}
            return records_iter, new_offset

        def _fetch_report_data_incremental(
            self, table_name: str, start_date: str
        ) -> tuple[Iterator[Dict], str]:
            """
            Fetch incremental data for a report using pagination.

            Args:
                table_name: Name of the report type
                start_date: Start date for this incremental fetch (YYYY-MM-DD or relative)

            Returns:
                Tuple of (records iterator, max_date_seen)
            """
            config = self._report_configs[table_name]

            # Collect all records first to determine max_date
            all_records = []
            max_date_seen = None

            offset = 0
            limit = 100000  # Max per request
            has_more = True

            while has_more:
                # Refresh token if needed
                self._refresh_token_if_needed()

                # Build request body
                request_body = {
                    "dateRanges": [
                        {"startDate": start_date, "endDate": self.end_date}
                    ],
                    "dimensions": config["dimensions"],
                    "metrics": config["metrics"],
                    "limit": limit,
                    "offset": offset,
                    "keepEmptyRows": False,
                }

                # Make API request
                url = f"{self.base_url}/properties/{self.property_id}:runReport"
                headers = {
                    "Authorization": f"Bearer {self.access_token}",
                    "Content-Type": "application/json",
                }

                response = requests.post(url, headers=headers, json=request_body)

                if response.status_code != 200:
                    raise Exception(
                        f"GA4 API error for {table_name}: {response.status_code} {response.text}"
                    )

                data = response.json()

                # Extract dimension and metric headers
                dimension_headers = [h["name"] for h in data.get("dimensionHeaders", [])]
                metric_headers = [h["name"] for h in data.get("metricHeaders", [])]

                # Process rows
                rows = data.get("rows", [])

                if not rows:
                    break

                for row in rows:
                    # Transform row to dict
                    record = self._transform_row(
                        row, dimension_headers, metric_headers, config
                    )

                    # Track max date for offset
                    if "date" in record:
                        date_value = record["date"]
                        if max_date_seen is None or date_value > max_date_seen:
                            max_date_seen = date_value

                    all_records.append(record)

                # Check if there are more pages
                row_count = len(rows)
                if row_count < limit:
                    has_more = False
                else:
                    offset += limit

                # Rate limiting - be nice to the API
                time.sleep(0.1)

            # Return iterator over collected records and max date
            return iter(all_records), max_date_seen or start_date

        def _transform_row(
            self,
            row: Dict,
            dimension_headers: List[str],
            metric_headers: List[str],
            config: Dict,
        ) -> Dict:
            """
            Transform a GA4 API row to match our schema.

            Args:
                row: Raw row from GA4 API
                dimension_headers: List of dimension names
                metric_headers: List of metric names
                config: Report configuration

            Returns:
                Transformed record
            """
            record = {}

            # Extract dimension values
            dimension_values = row.get("dimensionValues", [])
            for i, dim_name in enumerate(dimension_headers):
                if i < len(dimension_values):
                    record[dim_name] = dimension_values[i].get("value")

            # Extract metric values
            metric_values = row.get("metricValues", [])
            for i, metric_name in enumerate(metric_headers):
                if i < len(metric_values):
                    value_str = metric_values[i].get("value")

                    # Convert to appropriate type
                    if metric_name in [
                        "activeUsers",
                        "sessions",
                        "screenPageViews",
                        "eventCount",
                        "conversions",
                        "newUsers",
                    ]:
                        # Integer metrics
                        try:
                            record[metric_name] = int(value_str) if value_str else None
                        except (ValueError, TypeError):
                            record[metric_name] = None
                    else:
                        # Decimal metrics (rates, durations, revenue)
                        try:
                            record[metric_name] = float(value_str) if value_str else None
                        except (ValueError, TypeError):
                            record[metric_name] = None

            # Generate composite key
            key_parts = []
            for key_field in config["primary_key"]:
                value = record.get(key_field, "")
                # Handle null/empty values
                key_parts.append(str(value) if value else "(not set)")

            record["_composite_key"] = "|".join(key_parts)

            return record

        def _subtract_days(self, date_str: str, days: int) -> str:
            """
            Subtract N days from a date.

            Args:
                date_str: Date string in YYYYMMDD format
                days: Number of days to subtract

            Returns:
                Date minus N days in YYYYMMDD format
            """
            # Parse YYYYMMDD format
            year = int(date_str[:4])
            month = int(date_str[4:6])
            day = int(date_str[6:8])

            # Create date object and subtract days
            date_obj = datetime(year, month, day)
            earlier_date = date_obj - timedelta(days=days)

            # Return in YYYYMMDD format
            return earlier_date.strftime("%Y%m%d")

        def _increment_date(self, date_str: str) -> str:
            """
            Increment a date by one day.

            Args:
                date_str: Date string in YYYYMMDD format

            Returns:
                Next day in YYYYMMDD format
            """
            # Parse YYYYMMDD format
            year = int(date_str[:4])
            month = int(date_str[4:6])
            day = int(date_str[6:8])

            # Create date object and add one day
            date_obj = datetime(year, month, day)
            next_day = date_obj + timedelta(days=1)

            # Return in YYYYMMDD format
            return next_day.strftime("%Y%m%d")

        def test_connection(self) -> Dict:
            """
            Test the connection to GA4 API.

            Returns:
                Dictionary with status and message
            """
            try:
                # Refresh token if needed
                self._refresh_token_if_needed()

                # Try to get metadata about the property
                url = f"{self.base_url}/properties/{self.property_id}/metadata"
                headers = {"Authorization": f"Bearer {self.access_token}"}

                response = requests.get(url, headers=headers)

                if response.status_code == 200:
                    return {"status": "success", "message": "Connection successful"}
                else:
                    return {
                        "status": "error",
                        "message": f"API error: {response.status_code} {response.text}",
                    }
            except Exception as e:
                return {"status": "error", "message": f"Connection failed: {str(e)}"}


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            records, offset = self.lakeflow_connect.read_table(
                self.options["tableName"], start, self.options
            )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(table, self.options)
                all_records.append({"tableName": table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options["tableName"]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField("tableName", StringType(), False),
                        StructField("primary_key", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)
